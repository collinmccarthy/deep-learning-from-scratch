{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Softmax and Cross Entropy from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See [Blog (Jay Mody): Numerically Stable Softmax and Cross Entropy](https://jaykmody.com/blog/stable-softmax/) for nice write-up\n",
    "- Using `dim=1` by default because for multi-dimensional inputs, pytorch expects the channel dimension in dim=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 39.1881, -34.0590, -27.0345,  ..., -36.3017,  18.9438, -27.8977],\n",
      "        [-36.6272,   2.8195,  17.4534,  ...,  41.3051, -39.7945, -44.1213],\n",
      "        [ 29.3098,  -9.7967,  38.6168,  ...,  31.2592,  31.0836, -22.3497],\n",
      "        ...,\n",
      "        [ -9.5909, -36.8363,  24.4595,  ..., -20.8998,   3.5788, -29.3600],\n",
      "        [ 28.8947, -48.7126, -35.8005,  ...,  27.6717,  15.4610, -30.0755],\n",
      "        [ 16.3976, -26.9319,  12.1767,  ..., -32.3698,  10.6299,  39.5487]])\n",
      "y: tensor([23, 21,  4,  5, 30, 32, 62, 38, 51, 36, 53,  5, 32, 31, 35, 29])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Random tensor, shape (batch_size, num_classes)\n",
    "batch_size = 16\n",
    "num_classes = 64\n",
    "\n",
    "x_np = np.random.rand(batch_size, num_classes).astype(np.float32) * 100 - 50  # U(0,1] -> U(-50,50]\n",
    "y_np = np.random.randint(low=0, high=num_classes, size=(batch_size,))  # [low,high-1]\n",
    "\n",
    "# OR\n",
    "#   x_np = torch.rand((batch_size, num_classes), dtype=torch.float64).numpy()  # Differnet API as np\n",
    "#   y_np = torch.randint(low=0, high=num_classes, size=(batch_size,))  # Same API as np\n",
    "\n",
    "x_pt = torch.tensor(x_np)\n",
    "y_pt = torch.tensor(y_np)\n",
    "\n",
    "print(f\"x: {x_pt}\")\n",
    "print(f\"y: {y_pt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "- Definition: $\\text{softmax}(x)_k = \\frac{\\exp^{x_k}}{\\sum_j \\exp^{x_j}}$\n",
    "- Stable version:\n",
    "    - Multiply by some constant $\\frac{C}{C}$\n",
    "    - $\\text{softmax}(x)_k = \\frac{\\exp^{x_k}}{\\sum_j \\exp^{x_j}}$\n",
    "    - $\\text{softmax}(x)_k = \\frac{C \\cdot \\exp^{x_k}}{C \\cdot \\sum_j \\exp^{x_j}}$\n",
    "    - $\\text{softmax}(x)_k = \\frac{\\exp^{x_k + \\log C}}{\\sum_j \\exp^{x_j + \\log C}}$\n",
    "    - Let $\\log C = -\\max(x)$\n",
    "    - $\\text{softmax}(x)_k = \\frac{\\exp^{x_k -\\max(x)}}{\\sum_j \\exp^{x_j -\\max(x)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "smax_pt = F.softmax(x_pt, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allclose torch: True\n",
      "Allclose numpy: True\n"
     ]
    }
   ],
   "source": [
    "def softmax_torch(logits: torch.Tensor, dim: int = 1) -> torch.Tensor:\n",
    "    # logits.shape: (batch_size, vocab_size) = (N,K)\n",
    "    \n",
    "    # Stable softmax to get probs\n",
    "    #   >> softmax(x)_k = e^(x_k - max(x)) / sum_j e^(x_j - max(x))    \n",
    "    \n",
    "    # Subtract max for stability, then take exponential\n",
    "    #   torch.amax(x) doesn't return indices; torch.max(x, keepdim=True) returns (vals, ind)\n",
    "    exp_logits = torch.exp(logits - torch.amax(logits, dim=dim, keepdim=True))    \n",
    "    \n",
    "    # Normalize to turn into probs\n",
    "    probs = exp_logits / torch.sum(exp_logits, dim=dim, keepdim=True)\n",
    "    \n",
    "    return probs  # (N,K) -> (N,K)\n",
    "\n",
    "def softmax_numpy(logits: np.ndarray, dim: int = 1) -> np.ndarray:\n",
    "    # Numpy API differences here:\n",
    "    #   - `keepdims` instead of `keepdim`\n",
    "    #   - `axis` instead of `dim`\n",
    "    exp_logits = np.exp(logits - logits.max(axis=dim, keepdims=True))\n",
    "    probs = exp_logits / np.sum(exp_logits, axis=dim, keepdims=True)    \n",
    "    return probs\n",
    "\n",
    "smax_ours_pt = softmax_torch(x_pt, dim=1)\n",
    "smax_ours_np = softmax_numpy(x_np, dim=1)\n",
    "print(f\"Allclose torch: {torch.allclose(smax_pt, smax_ours_pt)}\")\n",
    "print(f\"Allclose numpy: {np.allclose(smax_pt.numpy(), smax_ours_np)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Definition: $H(p, q) = - \\sum_i p_i \\cdot \\log q_i$ where $p,q$ are prob. distributions\n",
    "    - If $p_i == 1 iff i == y else 0$\n",
    "        - $H(p, q) = - p_y \\cdot \\log q_y$\n",
    "        - $H(p, q) = - \\log q_y$\n",
    "        - $H(p, q) = - \\log \\text{softmax}(x)_y$\n",
    "- Plug in our definition of stable softmax\n",
    "    - $H(p, q) = - \\log \\text{softmax}(x)_y$\n",
    "    - $H(p, q) = - \\log \\frac{\\exp^{x_y -\\max(x)}}{\\sum_j \\exp^{x_j -\\max(x)}}$\n",
    "    - $H(p, q) = - (\\log (\\exp^{x_y -\\max(x)}) - \\log \\sum_j \\exp^{x_j -\\max(x)})$\n",
    "    - $H(p, q) = - (x_y -\\max(x) - \\log \\sum_j \\exp^{x_j -\\max(x)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(44.4969)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ce_pt = F.cross_entropy(x_pt, y_pt)\n",
    "ce_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allclose torch v1: True\n",
      "Allclose torch v2: True\n",
      "Allclose numpy v1: True\n",
      "Allclose numpy v2: True\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_torch_v1(logits: torch.Tensor, targets: torch.Tensor):\n",
    "    # logits.shape: (N,K) for (batch_size, num_classes)\n",
    "    # targets.shpae: (N,) for (num_classes,)\n",
    "    \n",
    "    # Via stable cross entropy\n",
    "    #   >>> - log(softmax(x)_k)  where k == correct class\n",
    "    #   >>> - log( (e^(x_k - max(x)) / sum_j e^(x_j - max(x)) )  \n",
    "    #   >>> - (x_k - max(x) - log( sum_j e^(x_j - max(x)) ))\n",
    "    #   >>> - x_k + max(x) + log( sum_j e^(x_j - max(x)) ))    \n",
    "    max_logits = torch.amax(logits, dim=1, keepdim=True)  # (N,K) -> (N,K)\n",
    "    log_sum = torch.log(torch.sum(torch.exp(logits - max_logits), dim=1))  # (N,K) -> (N,)\n",
    "    logits_y = logits[torch.arange(logits.shape[0], device=logits.device), targets]  # (N,K) -> (N,)\n",
    "    ce = torch.mean(-logits_y + max_logits.squeeze(dim=1) + log_sum)\n",
    "    return ce\n",
    "\n",
    "def cross_entropy_numpy_v1(logits: np.ndarray, targets: np.ndarray):\n",
    "    # Numpy API differences here:\n",
    "    #   - `keepdims` instead of `keepdim`\n",
    "    #   - `axis` instead of `dim`\n",
    "    max_logits = np.max(logits, axis=1, keepdims=True)  # (N,K) -> (N,K)\n",
    "    log_sum = np.log(np.sum(np.exp(logits - max_logits), axis=1))  # (N,K) -> (N,)    \n",
    "    logits_y = logits[np.arange(logits.shape[0]), targets]  # (N,K) -> (N,)\n",
    "    ce = np.mean(-logits_y + max_logits.squeeze(axis=1) + log_sum)\n",
    "    return ce\n",
    "\n",
    "def cross_entropy_torch_v2(logits: torch.Tensor, targets: torch.Tensor):\n",
    "    # logits.shape: (N,K) for (batch_size, num_classes)\n",
    "    # targets.shpae: (N,) for (num_classes,)\n",
    "    \n",
    "    # Via cross_entropy of stable softmax\n",
    "    #   >>> - log(softmax(x)_k)  where k == correct class\n",
    "    smax = softmax_torch(logits, dim=1)\n",
    "    smax_y = smax[torch.arange(logits.shape[0]), targets]  # (N,K) -> (N,)\n",
    "    ce = torch.mean(-torch.log(smax_y))\n",
    "    return ce\n",
    "\n",
    "def cross_entropy_numpy_v2(logits: np.ndarray, targets: np.ndarray):\n",
    "    # No numpy differences here besides `torch` -> `np`\n",
    "    #   - Only b/c we're using `softmax_numpy`\n",
    "    #   - Otherwise same `keepdim` -> `keepdims` and `dim` -> `axis` would apply\n",
    "    smax = softmax_numpy(logits, dim=1)\n",
    "    smax_y = smax[np.arange(logits.shape[0]), targets]  # (N,K) -> (N,)\n",
    "    ce = np.mean(-np.log(smax_y))\n",
    "    return ce\n",
    "\n",
    "ce_ours_torch_v1 = cross_entropy_torch_v1(x_pt, y_pt)  # Pass in x_pt, y_pt for torch.Tensors\n",
    "ce_ours_torch_v2 = cross_entropy_torch_v2(x_pt, y_pt)  # Pass in x_pt, y_pt for torch.Tensors\n",
    "\n",
    "ce_ours_numpy_v1 = cross_entropy_numpy_v1(x_np, y_np)  # Pass in x_np, y_np for np.ndarrays\n",
    "ce_ours_numpy_v2 = cross_entropy_numpy_v2(x_np, y_np)  # Pass in x_np, y_np for np.ndarrays\n",
    "\n",
    "print(f'Allclose torch v1: {torch.allclose(ce_pt, ce_ours_torch_v1)}')\n",
    "print(f'Allclose torch v2: {torch.allclose(ce_pt, ce_ours_torch_v2)}')\n",
    "print(f'Allclose numpy v1: {numpy.allclose(ce_pt, ce_ours_numpy_v1)}')\n",
    "print(f'Allclose numpy v2: {numpy.allclose(ce_pt, ce_ours_numpy_v2)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
