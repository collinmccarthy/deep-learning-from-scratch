{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch\n",
    "\n",
    "- From [YouTube (Andrej Karpathy): Let's build GPT: from scratch, in code, spelled out.](https://youtu.be/kCc8FmEb1nY?si=rrytCRSJeL4jaCJt)\n",
    "- Refactored / trimmed down version of [Google colab notebook for video](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)\n",
    "    - Functionally the same\n",
    "    - Extra comments here as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-06 09:47:44--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2024-12-06 09:47:44 (15.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset (Implicit Tokenizer)\n",
    "\n",
    "- Implicitly building tokenizer via `stoi` and `itos`\n",
    "    - Real examples: \n",
    "        - [google/sentencepiece](https://github.com/google/sentencepiece)\n",
    "        - [openai/tiktoken](https://github.com/openai/tiktoken)\n",
    "    - Both use BPE (byte-pair encoding)\n",
    "- Workflow is as follows:\n",
    "    - Data is read in and stored as vocab: `chars` (len=65)\n",
    "        - Build lookup function `stoi` (char to int)\n",
    "        - Build lookup function `itos` (int to char)\n",
    "    - Build encoder function `encode: str -> list[int]`, using `stoi` (char to int)\n",
    "    - Build decoder function `decode: list[int] -> str`, using `itos` (int to char) then concat\n",
    "    - Build dataset `data` by simply encoding all characters in our dataset\n",
    "        - Split into train (90%), val (10%)\n",
    "    - Build dataloaders from `data` splits via random mini-batches (infinite dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab (size 65): \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Data tensor shape:\n",
      "  torch.Size([1115394]), type: torch.int64\n",
      "Data tensor example (100 chars):\n",
      "  tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
      "Encode example:\n",
      "  encode(\"hii there\") == [46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "Decode example:\n",
      "  decode(encode(\"hii there\")) == hii there\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(\n",
    "    data_path: str, \n",
    "    verbose: bool = False\n",
    ") -> tuple[torch.Tensor, torch.Tensor, Callable, Callable, int]:\n",
    "    # Read input.txt\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Create vocab\n",
    "    chars = sorted(list(set(text)))\n",
    "    vocab_size = len(chars)\n",
    "\n",
    "    # Create a mapping from characters to integers\n",
    "    # This is our \"tokenizer\"\n",
    "    stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "    itos = { i:ch for i,ch in enumerate(chars) }\n",
    "    \n",
    "    # Tokenizer encoder: take a string, output a list of integers\n",
    "    def encode(s: str) -> list[int]:\n",
    "        return [stoi[c] for c in s] \n",
    "    \n",
    "    # Tokenizer decoder: take a list of integers, output a string\n",
    "    def decode(l: list[int]) -> str:\n",
    "        return ''.join([itos[i] for i in l]) \n",
    "\n",
    "    # Encode entire dataset into tensor\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "    # Create splits: 90% train, 10% val\n",
    "    n_train = int(0.9 * len(data))\n",
    "    train_data = data[:n_train]\n",
    "    val_data = data[n_train:]\n",
    "\n",
    "    # Print some useful things\n",
    "    if verbose:\n",
    "        print(f'Vocab (size {vocab_size}): {\"\".join(chars)}')\n",
    "        print(f'Data tensor shape:\\n  {data.shape}, type: {data.dtype}')\n",
    "        print(f'Data tensor example (100 chars):\\n  {data[:100]}')\n",
    "        print(f'Encode example:\\n  encode(\"hii there\") == {encode(\"hii there\")}')\n",
    "        print(f'Decode example:\\n  decode(encode(\"hii there\")) == {decode(encode(\"hii there\"))}')\n",
    "    \n",
    "    return train_data, val_data, encode, decode, vocab_size\n",
    "\n",
    "train_data, val_data, _encode, _decode, _vocab_size = build_dataset(\n",
    "    data_path='input.txt', verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfiniteDataLoader:\n",
    "    def __init__(self, data: torch.Tensor, batch_size: int, block_size: int, seed: int):\n",
    "        self.data: torch.Tensor = data\n",
    "        self.batch_size: int = batch_size  # e.g. 4\n",
    "        self.block_size: int = block_size  # context/seq length, e.g. 8\n",
    "        self.gen: torch.Generator = torch.Generator().manual_seed(seed)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        ix = torch.randint(  # Shape (4,)\n",
    "            # Max int is len(data) - block_size because we will use i -> i+block_size \n",
    "            len(self.data) - self.block_size, (self.batch_size,), \n",
    "            generator=self.gen,\n",
    "        )  \n",
    "        x = [self.data[i:i+self.block_size] for i in ix]  # List of tensors, 4x shape(8,)\n",
    "        y = [self.data[i+1:i+self.block_size+1] for i in ix]  # List of tensors, 4x shape(8,)\n",
    "        x = torch.stack(x)  # 4x (8,) -> (4,8)\n",
    "        y = torch.stack(y)  # 4x (8,) -> (4,8)\n",
    "        return x, y\n",
    "            \n",
    "def build_dataloaders(\n",
    "    train_data: torch.Tensor,\n",
    "    val_data: torch.Tensor,\n",
    "    batch_size: int, \n",
    "    block_size: int, # context/seq length\n",
    "    seed: int = 1337,    \n",
    ") -> tuple[InfiniteDataLoader, InfiniteDataLoader]:\n",
    "    \n",
    "    train_dataloader = InfiniteDataLoader(\n",
    "        data=train_data, batch_size=batch_size, block_size=block_size, seed=seed,\n",
    "    )\n",
    "    val_dataloader = InfiniteDataLoader(\n",
    "        data=val_data, batch_size=batch_size, block_size=block_size, seed=seed,\n",
    "    )\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 inputs (shape torch.Size([4, 8])):\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "batch 0 targets (shape torch.Size([4, 8])), aka inputs shifted right one:\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "batch 0 training sequence:\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8  # context/seq length\n",
    "train_dataloader, val_dataloader = build_dataloaders(\n",
    "    train_data=train_data, val_data=val_data, batch_size=batch_size, block_size=block_size\n",
    ")\n",
    "\n",
    "xb, yb = next(iter(train_dataloader))\n",
    "print(f'batch 0 inputs (shape {xb.shape}):')\n",
    "print(xb)\n",
    "print(f'batch 0 targets (shape {yb.shape}), aka inputs shifted right one:')\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "print('batch 0 training sequence:')\n",
    "for b in range(1): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    \"\"\" one head of self-attention \n",
    "    \n",
    "    Previously called 'Head'\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, head_size: int, block_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class SimpleMultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \n",
    "    \n",
    "    Previously called 'MultiHeadAttention'\n",
    "    \n",
    "    This is the \"easy\" way, where we just repeat this in parallel, concatenate the result, and\n",
    "    add an output projection. The \"better\" way is to combine the W_k, W_q, W_v matrices per head,\n",
    "    as well as the attn calculation, to speed things up a lot.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, num_heads: int, head_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                SingleHeadAttention(n_embd=n_embd, head_size=head_size, block_size=block_size, dropout=dropout) \n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_head: int, dropout: float):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(\n",
    "            n_embd=n_embd, num_heads=n_head, head_size=head_size, dropout=dropout\n",
    "        )\n",
    "        self.ffwd = FeedFoward(n_embd=n_embd, dropout=dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))  # Pre-norm (more common now; orig paper was post-norm)\n",
    "        x = x + self.ffwd(self.ln2(x))  # Pre-norm (more common now; orig paper was post-norm)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Bigram Model w/ Transformer Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int, \n",
    "        n_embd: int, \n",
    "        block_size: int, \n",
    "        n_head: int, \n",
    "        n_layer: int, \n",
    "        dropout: float,\n",
    "        loss_fn: Callable = F.cross_entropy,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  # No max seq. length here\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # Max seq length here\n",
    "        blocks = [\n",
    "            Block(n_embd=n_embd, n_head=n_head, dropout=dropout) \n",
    "            for _ in range(n_layer)\n",
    "        ]\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        # print the number of parameters in the model\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f'{self.__class__.__name__}: {num_params / 1e6: .2f} M parameters')\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=tok_emb.device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # Big picture: take (B,T), generate new tokens, e.g. generate((B,T)) -> (B,T+1) and so on\n",
    "        #   - Forward pass: (B,K) -> (B,L,K)\n",
    "        #   - Last token (B,-1,K) gives logits for next token\n",
    "        #   - Apply softmax to get probs\n",
    "        #   - Use torch.multinomial to sample next token index\n",
    "        #   - Append next token (B,K) -> (B,K+1)\n",
    "        #   - Crop sequence to last K tokens\n",
    "        #   - Repeat\n",
    "        \n",
    "        # Batch generate\n",
    "        #   idx.shape: (B,T) where B = batch_size, T = num_tokens (seq len)\n",
    "        # Here T doesn't have to be block_size == max context len, can just be 1\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens (if T < block_size this is just T)\n",
    "            idx_cond = idx[:, -self.block_size:]  # (B,T) -> (B,L)\n",
    "            # get the predictions\n",
    "            logits, _loss = self(idx_cond, targets=None)  # (B,L) -> (B,L,C)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # (B,L,C) -> (B,C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,C) -> (B,1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B,T) -> (B,T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(\n",
    "    model: nn.Module, \n",
    "    train_dataloader: InfiniteDataLoader, \n",
    "    val_dataloader: InfiniteDataLoader, \n",
    "    eval_iters: int,\n",
    "    device: str,\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split, dataloader in [('train', train_dataloader), ('val', val_dataloader)]:       \n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k, (X,Y) in enumerate(dataloader):\n",
    "            # Move data to device (usually part of collate_fn but not implemented in InfiniteDataLoader)\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            _logits, loss = model(X, Y)\n",
    "            \n",
    "            losses[k] = loss.item()\n",
    "            if k+1 >= eval_iters:  # b/c infinite dataloader\n",
    "                break\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramLanguageModel:  0.21 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:  10%|█         | 505/5000 [00:15<12:20,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [500/5000]: train_loss=2.3012, val_loss=2.3193, lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:  20%|██        | 1009/5000 [00:29<09:03,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [1000/5000]: train_loss=2.0963, val_loss=2.1303, lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:  30%|███       | 1506/5000 [00:44<11:44,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [1500/5000]: train_loss=1.9637, val_loss=2.0396, lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:  40%|████      | 2005/5000 [00:58<07:01,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [2000/5000]: train_loss=1.8625, val_loss=1.9771, lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:  50%|█████     | 2508/5000 [01:13<07:12,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [2500/5000]: train_loss=1.7999, val_loss=1.9254, lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:  60%|██████    | 3009/5000 [01:27<04:43,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [3000/5000]: train_loss=1.7508, val_loss=1.9026, lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:  70%|███████   | 3507/5000 [01:41<03:54,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [3500/5000]: train_loss=1.7308, val_loss=1.8715, lr=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:  80%|████████  | 4006/5000 [01:56<02:37,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [4000/5000]: train_loss=1.6916, val_loss=1.8336, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter:  90%|█████████ | 4507/5000 [02:10<01:17,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [4500/5000]: train_loss=1.6358, val_loss=1.7833, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iter: 100%|█████████▉| 4999/5000 [02:25<00:00, 34.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter [5000/5000]: train_loss=1.6227, val_loss=1.7835, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------\n",
    "# moodel hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "# train hyperparams\n",
    "max_iters = 5000\n",
    "print_iters = None  # max_iters // 20; skipping, just using eval_iterval\n",
    "eval_interval = max_iters // 10\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "data_path = 'input.txt'\n",
    "seed = 1337\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# ------------\n",
    "\n",
    "# Build dataset and tokenizer\n",
    "train_data, val_data, _encode, decode, vocab_size = build_dataset(data_path=data_path)\n",
    "\n",
    "# Build dataloaders\n",
    "train_dataloader, val_dataloader = build_dataloaders(\n",
    "    train_data=train_data, \n",
    "    val_data=val_data, \n",
    "    batch_size=batch_size, \n",
    "    block_size=block_size, \n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "# Build model and move to device\n",
    "model = BigramLanguageModel(\n",
    "    vocab_size=vocab_size, \n",
    "    n_embd=n_embd, \n",
    "    block_size=block_size, \n",
    "    n_head=n_head, \n",
    "    n_layer=n_layer, \n",
    "    dropout=dropout\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Build optimizer and scheduler\n",
    "reduce_lr_steps = max(1, int(max_iters * 0.8))\n",
    "optimizer = optim.AdamW(params=model.parameters(), lr=learning_rate)  # pyright: ignore[reportPrivateImportUsage]\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=reduce_lr_steps, gamma=0.1)\n",
    "\n",
    "for idx, (xb, yb) in tqdm(enumerate(train_dataloader), desc=\"iter\", total=max_iters):\n",
    "    \n",
    "    # Move data to device (usually part of collate_fn but not implemented in InfiniteDataLoader)\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    logits, loss = model(xb, yb)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print loss on current batch (skipped if print_iters is None)\n",
    "    if print_iters is not None and (idx+1) % print_iters == 0:\n",
    "        print(\n",
    "            f\"Iter [{idx+1}/{max_iters}]:\"\n",
    "            f\" loss={loss.item():.3f}, lr={optimizer.param_groups[0]['lr']}\"\n",
    "        )\n",
    "    \n",
    "    # Print loss / mini-evaluate\n",
    "    if (eval_interval is not None and (idx+1) % eval_interval == 0) or (idx+1) == max_iters:\n",
    "        losses = estimate_loss(\n",
    "            model=model, \n",
    "            train_dataloader=train_dataloader, \n",
    "            val_dataloader=val_dataloader, \n",
    "            eval_iters=eval_iters,\n",
    "            device=device,\n",
    "        )\n",
    "        print(\n",
    "            f\"Iter [{idx+1}/{max_iters}]:\"\n",
    "            f\" train_loss={losses['train']:.4f},\"\n",
    "            f\" val_loss={losses['val']:.4f},\"\n",
    "            f\" lr={optimizer.param_groups[0]['lr']}\"\n",
    "        )\n",
    "    \n",
    "    if idx+1 >= max_iters:\n",
    "        break  # b/c infinite dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Come but I am on joy, Bukly be a their awomer:\n",
      "Here conferrows weeps.\n",
      "\n",
      "MERCUTIO:\n",
      "How nother:\n",
      "Mare before.\n",
      "\n",
      "LADY ANNE:\n",
      "Make to accoudio, their a grant me lord.\n",
      "Indep you spriect?\n",
      "\n",
      "FLORIZARET:\n",
      "To the k\n"
     ]
    }
   ],
   "source": [
    "# Generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Zero is newline char (reasonable start token)\n",
    "generated = model.generate(context, max_new_tokens=200)  # (1,1) -> (1,200)\n",
    "generated_txt = decode(generated[0].tolist())  # (1,200) -> (200,) -> list -> decode int to char\n",
    "print(generated_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
