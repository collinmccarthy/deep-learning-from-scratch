{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNNs from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code based on [GitHub: TheIndependentCode/Neural-Network](https://github.com/TheIndependentCode/Neural-Network)\n",
    "    - Also see [YouTube (TheIndependentCode): Neural Networks](https://youtube.com/playlist?list=PLQ4osgQ7WN6PGnvt6tzLAVAEMsL3LBqpm&si=nYp4J9dIP13Bcaxs)\n",
    "        - Accompanies GitHub repo\n",
    "\n",
    "- Following [YouTube (AI with Frank): \\[Tutorial\\] Convolutional layers implementations under the hood](https://youtu.be/-Y4ST8eNySI?si=1L7VuFRZPsFxkPnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (N, C_in, H_in, W_in) = (1, 3, 4, 4)\n",
      "weight.shape: (C_out, C_in, K, K) = (1, 3, 2, 2)\n",
      "bias.shape: (C_out,) = (1,)\n",
      "Calculated (H_out, W_out) = (3, 3)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyperparams\n",
    "# N, C_in, H_in, W_in = (2, 3, 4, 4)\n",
    "N, C_in, H_in, W_in = (1, 3, 4, 4)\n",
    "K = 2\n",
    "S = 1\n",
    "C_out = 1\n",
    "seed = 1337\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(seed)\n",
    "torch.random.manual_seed(seed)\n",
    "\n",
    "x_np = np.random.rand(N, C_in, H_in, W_in).astype(np.float32)\n",
    "x_pt = torch.tensor(x_np)\n",
    "\n",
    "# Alternative: pytorch init\n",
    "# x_pt = torch.tensor((N, C_in, H_in, W_in), dtype=torch.float32)\n",
    "# x_np = x_pt.numpy()\n",
    "\n",
    "# Let's cheat for comparison sake: create our nn layer and steal weights from that\n",
    "conv = nn.Conv2d(in_channels=C_in, out_channels=C_out, kernel_size=K, stride=S, bias=True)\n",
    "weight_pt = conv.weight.detach().clone()\n",
    "bias_pt = conv.bias.detach().clone()\n",
    "\n",
    "weight_np = weight_pt.numpy()\n",
    "bias_np = bias_pt.numpy()\n",
    "\n",
    "print(f\"x.shape: (N, C_in, H_in, W_in) = {tuple(x_pt.shape)}\")\n",
    "print(f\"weight.shape: (C_out, C_in, K, K) = {tuple(conv.weight.shape)}\")\n",
    "print(f\"bias.shape: (C_out,) = {tuple(conv.bias.shape)}\")\n",
    "\n",
    "# Calculate output shapes (used below)\n",
    "# H_out = floor(H-K/s) + 1   (same for W_out)\n",
    "# e.g. 4x4, K=2, S=1 -> floor((4-2)/1) + 1 -> 3x3\n",
    "# e.g. 4x4, K=3, S=1 -> floor((4-3)/1) + 1 -> 2x2\n",
    "# e.g. 4x4, K=2, S=3 -> floor((4-2)/3) + 1 -> 1x1\n",
    "H_out = math.floor((H_in - K) / S) + 1\n",
    "W_out = math.floor((W_in - K) / S) + 1\n",
    "print(f\"Calculated (H_out, W_out) = {H_out, W_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: (N, C_out, H_out, W_out) = (1, 1, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "# PyTorch module\n",
    "with torch.no_grad():\n",
    "    out_pt = conv(x_pt)\n",
    "    print(f\"out.shape: (N, C_out, H_out, W_out) = {tuple(out_pt.shape)}\")\n",
    "    assert out_pt.shape == (N, C_out, H_out, W_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allclose: True\n"
     ]
    }
   ],
   "source": [
    "# PyTorch functional\n",
    "with torch.no_grad():\n",
    "    out_pt_func = F.conv2d(input=x_pt, weight=weight_pt, bias=bias_pt, stride=S)\n",
    "    print(f\"All equal: {torch.all(out_pt == out_pt_func)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allclose: True\n"
     ]
    }
   ],
   "source": [
    "# Naive loop\n",
    "#   x.shape: (N, C_in, H_in, W_in) = (1, 3, 4, 4)\n",
    "#   weight.shape: (C_out, C_in, K, K) = (6, 3, 2, 2)\n",
    "#   bias.shape: (C_out,) = (6,)\n",
    "#   out.shape: (N, C_out, H_out, W_out) = (1, 6, 3, 3)\n",
    "\n",
    "# Rem: H_out = floor(H-K/s) + 1\n",
    "#   e.g. 4x4, K=2, S=1 -> floor((4-2)/1) + 1 -> 3x3\n",
    "# At output location i, input location is i*s -> i*s\n",
    "\n",
    "# Stride kernel over image, compute (C_in,K,K) dot (C_in,H',W') with input region H' x W'\n",
    "out_loop_np = np.zeros((N, C_out, H_out, W_out), dtype=np.float32)\n",
    "for n in range(N):\n",
    "    for c_out in range(C_out):\n",
    "        out_loop_np[n, c_out]\n",
    "        for y_out in range(H_out):\n",
    "            y_in = y_out * S\n",
    "            for x_out in range(W_out):\n",
    "                x_in = x_out * S\n",
    "                patch = x_np[n, :, y_in:y_in+K, x_in:x_in+K].flatten()  # (1,C_in,K,K)\n",
    "                kernel = weight_np[c_out].flatten()  # (C_in,K,K)\n",
    "                out_loop_np[n, c_out, y_out, x_out] = patch @ kernel + bias_np[c_out]\n",
    "\n",
    "print(f\"Allclose: {np.allclose(out_pt.numpy(), out_loop_np)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allclose numpy: True\n",
      "Allclose numpy: True\n"
     ]
    }
   ],
   "source": [
    "def conv2d_im2col(\n",
    "    x: np.ndarray | torch.Tensor, \n",
    "    weight: np.ndarray | torch.Tensor, \n",
    "    bias: np.ndarray | torch.Tensor, \n",
    "    stride: int, \n",
    "    backend: str = 'numpy') -> np.ndarray | torch.Tensor:\n",
    "    # - Compute X_out = X @ W  \n",
    "    #   - Need output for every dot product\n",
    "    #   - e.g. output size = N * C_out * H_out * W_out\n",
    "    # - W: rows = out channels, kernels = kernel patches\n",
    "    #   - Shape (C_out, C_in*K*K)\n",
    "    #   - Only requries view/reshape of original weights, no copy\n",
    "    # - X: rows = image patches, cols = output spatial locations\n",
    "    #   - Shape (C_in*K*K, N*H_out*W_out)\n",
    "    # - X_out: W @ X\n",
    "    #   - Shape (C_out, N*H_out*W_out)\n",
    "    #   - Permute/reshape to (N,C_out,H_out,W_out)\n",
    "    assert backend in ['numpy', 'torch']\n",
    "    \n",
    "    # Get shapes\n",
    "    N, C_in, H_in, W_in = x.shape\n",
    "    C_out, C_in, K, _K = weight.shape\n",
    "    assert bias.shape == (C_out,)\n",
    "    \n",
    "    H_out = math.floor((H_in - K) / S) + 1\n",
    "    W_out = math.floor((W_in - K) / S) + 1\n",
    "    \n",
    "    # Setup (use 'zeros' instead of 'empty' for debugging)\n",
    "    if backend == 'numpy':\n",
    "        X = np.empty((C_in * K * K, N * H_out * W_out), dtype=np.float32)\n",
    "        W = weight.reshape((C_out, C_in * K * K))\n",
    "        assert all([isinstance(a, np.ndarray) for a in [x, weight, bias]]), \\\n",
    "            \"Must pass in numpy arrays for backend == 'numpy'\"\n",
    "        assert W.base is not None, \"Expected a view for W not a copy\"  # pyright: ignore[reportAttributeAccessIssue]\n",
    "    elif backend == 'torch':\n",
    "        X = torch.empty((C_in * K * K, N * H_out * W_out), dtype=torch.float32)\n",
    "        W = weight.view((C_out, C_in * K * K))        \n",
    "        assert all([isinstance(t, torch.Tensor) for t in [x, weight, bias]]), \\\n",
    "            \"Must pass in torch tensors for backend == 'torch'\"\n",
    "\n",
    "    # im2col\n",
    "    for n in range(N):\n",
    "        for y_out in range(H_out):\n",
    "            y_in = y_out * stride\n",
    "            for x_out in range(W_out):\n",
    "                x_in = x_out * stride   \n",
    "                col = (n * H_out * W_out) + (y_out * W_out) + x_out\n",
    "                img_patch = x[n, :, y_in:y_in+K, x_in:x_in+K].flatten()  # patch, (1,C_in,K,K)\n",
    "                X[:, col] = img_patch  # pyright: ignore[reportArgumentType]\n",
    "                if isinstance(X, np.ndarray):  # backend == 'numpy'\n",
    "                    assert X[:, col].base is not None, \"Expected a view for X[i] not a copy\"\n",
    "\n",
    "    if backend == 'numpy':\n",
    "        assert isinstance(X, np.ndarray) and isinstance(W, np.ndarray)\n",
    "        \n",
    "        # Make contiguous; probably not necessary (numpy should do it) but good to be explicit\n",
    "        X = np.ascontiguousarray(X)\n",
    "\n",
    "        # (C_out, C_in*K*K) @ (C_in*K*K, N*H_out*W_out) -> (C_out, N*H_out*W_out)\n",
    "        out_im2col = W @ X + bias.reshape(C_out, 1)  # Explicit reshape (not required)\n",
    "        out_im2col = out_im2col.reshape(C_out, N, H_out, W_out)\n",
    "        assert out_im2col.base is not None, \"Expected a view for out_im2col_mat.reshape\"\n",
    "\n",
    "        out_im2col = np.ascontiguousarray(np.transpose(out_im2col, (1, 0, 2, 3)))        \n",
    "        \n",
    "    elif backend == 'torch':\n",
    "        assert isinstance(X, torch.Tensor) and isinstance(W, torch.Tensor)\n",
    "        X = X.contiguous()\n",
    "        \n",
    "        # (C_out, C_in*K*K) @ (C_in*K*K, N*H_out*W_out) -> (C_out, N*H_out*W_out)\n",
    "        out_im2col = W @ X + bias.view(C_out, 1)  # Explicit view (not required)  # pyright: ignore[reportArgumentType]\n",
    "        \n",
    "        # (C_out, N*H_out*W_out) -> (C_out, N, H_out, W_out) -> (N, C_out, H_out, W_out)\n",
    "        out_im2col = out_im2col.view(C_out, N, H_out, W_out).permute(1, 0, 2, 3).contiguous()\n",
    "        \n",
    "    return out_im2col\n",
    "        \n",
    "out_im2col_np = conv2d_im2col(x=x_np, weight=weight_np, bias=bias_np, stride=S, backend='numpy')\n",
    "print(f\"Allclose numpy: {np.allclose(out_pt.numpy(), out_im2col_np)}\")\n",
    "\n",
    "out_im2col_pt = conv2d_im2col(x=x_pt, weight=weight_pt, bias=bias_pt, stride=S, backend='torch')\n",
    "print(f\"Allclose numpy: {torch.allclose(out_pt, out_im2col_pt)}\")  # pyright: ignore[reportArgumentType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
